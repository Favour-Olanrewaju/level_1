{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3c0bfc22",
   "metadata": {},
   "source": [
    "# Day 21: Explanation of Handling Imbalanced Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "74e60069",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.98      0.97       275\n",
      "           1       0.76      0.64      0.70        25\n",
      "\n",
      "    accuracy                           0.95       300\n",
      "   macro avg       0.86      0.81      0.84       300\n",
      "weighted avg       0.95      0.95      0.95       300\n",
      "\n",
      "Cross-validation scores: [0.95       0.92857143 0.92857143 0.97857143 0.94285714]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Generate a synthetic imbalanced dataset\n",
    "X, y = make_classification(n_samples=1000, n_features=20, n_classes=2, \n",
    "                           weights=[0.9, 0.1], flip_y=0, random_state=42)\n",
    "\n",
    "# Split data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Train a RandomForestClassifier with balanced class weights\n",
    "rf = RandomForestClassifier(random_state=42, class_weight='balanced')\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model\n",
    "y_pred = rf.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Cross-validation on the balanced dataset\n",
    "cv_scores = cross_val_score(rf, X_train, y_train, cv=5, scoring='accuracy')\n",
    "print(\"Cross-validation scores:\", cv_scores)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cba2941e",
   "metadata": {},
   "source": [
    "1. Explanation of Handling Imbalanced Data\n",
    "Imbalanced data refers to a situation where the classes in a classification problem are not equally represented. For instance, one class might have far more samples than the other. This is common in real-world datasets, especially in fraud detection, disease diagnosis, and rare event prediction.\n",
    "\n",
    "When a dataset is imbalanced, machine learning algorithms tend to be biased towards the majority class. This can result in poor performance in predicting the minority class, which may be the class of interest.\n",
    "\n",
    "Techniques to Handle Imbalanced Data:\n",
    "Resampling Methods:\n",
    "\n",
    "Oversampling the Minority Class: This involves increasing the number of samples in the minority class (e.g., using techniques like SMOTE).\n",
    "Undersampling the Majority Class: This involves reducing the number of samples in the majority class to balance the class distribution.\n",
    "Algorithm-Level Approaches:\n",
    "\n",
    "Some machine learning algorithms like decision trees or random forests can be adjusted to account for class imbalance through the class_weight parameter (e.g., class_weight='balanced' in scikit-learn).\n",
    "Anomaly Detection: In cases where the imbalance is extreme, treating the problem as an anomaly detection problem can sometimes yield better results.\n",
    "\n",
    "Evaluation Metrics:\n",
    "\n",
    "Use metrics such as Precision, Recall, F1-Score, and AUC-ROC instead of accuracy, which can be misleading when the data is imbalanced.\n",
    "2. Importance of Handling Imbalanced Data\n",
    "Handling imbalanced data is crucial because:\n",
    "\n",
    "Bias towards Majority Class: Without addressing the imbalance, models may predict the majority class almost exclusively, leading to poor performance on the minority class.\n",
    "Real-world Relevance: In many cases, the minority class is more important (e.g., fraud detection, rare disease diagnosis), and itâ€™s crucial to develop models that can accurately predict it.\n",
    "Reliable Model Performance: By using the appropriate techniques, you ensure the model is evaluated on all classes and that the performance is not skewed by the class distribution.\n",
    "\n",
    "#100DaysOfCodeDay21 #ImbalancedData #MachineLearning"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
